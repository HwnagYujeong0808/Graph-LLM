{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess the BGM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Graph-LLM/code'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of CUDA devices: 2\n",
      "Current CUDA device: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Availability of CUDA \n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of CUDA devices:\", torch.cuda.device_count())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current CUDA device:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No CUDA device available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# !pip install transformers accelerate torch datasets\n",
    "\n",
    "# Import necessary libraries\n",
    "import json\n",
    "import pathlib\n",
    "import pickle\n",
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "import copy\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from accelerate import Accelerator\n",
    "from transformers import default_data_collator, AutoTokenizer, AutoModelForCausalLM\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>Gene1</th>\n",
       "      <th>Gene2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Gata1</td>\n",
       "      <td>Eklf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Fli1</td>\n",
       "      <td>Eklf</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Cebpa</td>\n",
       "      <td>Pu1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Pu1</td>\n",
       "      <td>Pu1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Gata1</td>\n",
       "      <td>Pu1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>895</td>\n",
       "      <td>Gfi1</td>\n",
       "      <td>cJun</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>896</td>\n",
       "      <td>Gata2</td>\n",
       "      <td>Gata2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>897</td>\n",
       "      <td>Pu1</td>\n",
       "      <td>Gata2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>898</td>\n",
       "      <td>Gata1</td>\n",
       "      <td>Gata2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>899</td>\n",
       "      <td>Fog1</td>\n",
       "      <td>Gata2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     node_id  Gene1  Gene2  label\n",
       "0          0  Gata1   Eklf      1\n",
       "1          1   Fli1   Eklf     -1\n",
       "2          2  Cebpa    Pu1      1\n",
       "3          3    Pu1    Pu1      1\n",
       "4          4  Gata1    Pu1     -1\n",
       "..       ...    ...    ...    ...\n",
       "895      895   Gfi1   cJun     -1\n",
       "896      896  Gata2  Gata2      1\n",
       "897      897    Pu1  Gata2     -1\n",
       "898      898  Gata1  Gata2     -1\n",
       "899      899   Fog1  Gata2     -1\n",
       "\n",
       "[900 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_df = pd.read_csv(\"../data/HCS/combined_graphs.csv\")\n",
    "gene_df['Type'] = gene_df['Type'].replace({'+': 1, '-': -1})\n",
    "\n",
    "# Rename columns\n",
    "gene_df.rename(columns={'Unnamed: 0': 'node_id', 'Type': 'label'}, inplace=True)\n",
    "\n",
    "gene_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge Type:\n",
      " 0    1\n",
      "1   -1\n",
      "2    1\n",
      "3    1\n",
      "4   -1\n",
      "Name: label, dtype: int64\n",
      "Unique Genes:['Gata1' 'Fli1' 'Cebpa' 'Pu1' 'Gata2' 'cJun' 'Gfi1' 'EgrNab' 'Fog1' 'Eklf'\n",
      " 'Scl']\n",
      "Number of Unique Genes: 11\n"
     ]
    }
   ],
   "source": [
    "# Generate unique node_ids for each unique gene\n",
    "unique_genes = pd.concat([gene_df['Gene1'], gene_df['Gene2']]).unique()\n",
    "node_id_map = {gene: idx for idx, gene in enumerate(unique_genes)}\n",
    "\n",
    "print(\"Edge Type:\\n\", gene_df['label'][:5])\n",
    "print(f\"Unique Genes:{unique_genes}\\nNumber of Unique Genes: {len(unique_genes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['node_id', 'Gene1', 'Gene2', 'label'],\n",
      "    num_rows: 900\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Convert to the required structure\n",
    "df = gene_df[['node_id', 'Gene1', 'Gene2', 'label']]\n",
    "\n",
    "# Convert the DataFrame to a Dataset object\n",
    "gene_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Print the features of the dataset to verify the transformation\n",
    "print(gene_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and Load the Tokenizer \n",
    "+ daryl149/llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.6.0+cu102.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# # Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('daryl149/llama-2-7b-chat-hf')\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(adapter_dim=768, adapter_len=5, adapter_n_heads=6, batch_size=16, dataset='mol', eval_batch_size=32, exp_num=1, grad_steps=2, lr=5e-05, model_name='LLaMA-7B-2', n_decoder_layers=4, n_encoder_layers=4, n_mp_layers=4, num_epochs=15, project='project_GraphLLM', rrwp=8, warmup_epochs=1, wd=0.1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "def parse_args_llama():\n",
    "    parser = argparse.ArgumentParser(description=\"GraphLLM\")\n",
    "\n",
    "    parser.add_argument(\"--project\", type=str, default=\"project_GraphLLM\")\n",
    "    parser.add_argument(\"--exp_num\", type=int, default=1)\n",
    "    parser.add_argument(\"--model_name\", type=str, default='LLaMA-7B-2')\n",
    "\n",
    "    parser.add_argument(\"--dataset\", type=str, default='mol')\n",
    "    parser.add_argument(\"--lr\", type=float, default=5e-5)\n",
    "    parser.add_argument(\"--wd\", type=float, default=0.1)\n",
    "\n",
    "    parser.add_argument(\"--adapter_len\", type=int, default=5)\n",
    "    parser.add_argument(\"--adapter_dim\", type=int, default=768)\n",
    "    parser.add_argument(\"--adapter_n_heads\", type=int, default=6)\n",
    "\n",
    "    parser.add_argument(\"--n_decoder_layers\", type=int, default=4)\n",
    "    parser.add_argument(\"--n_encoder_layers\", type=int, default=4)\n",
    "    parser.add_argument(\"--n_mp_layers\", type=int, default=4)\n",
    "\n",
    "    # Model Training\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=16)\n",
    "    parser.add_argument(\"--grad_steps\", type=int, default=2)\n",
    "\n",
    "    # Learning Rate Scheduler\n",
    "    parser.add_argument(\"--num_epochs\", type=int, default=15)\n",
    "    parser.add_argument(\"--warmup_epochs\", type=float, default=1)\n",
    "\n",
    "    # RRWP\n",
    "    parser.add_argument(\"--rrwp\", type=int, default=8)\n",
    "\n",
    "    # Inference\n",
    "    parser.add_argument(\"--eval_batch_size\", type=int, default=32)\n",
    "\n",
    "    # Jupyter 노트북에서 불필요한 인수 제거\n",
    "    if 'ipykernel_launcher' in sys.argv[0]:\n",
    "        sys.argv = sys.argv[:1]\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "# 인수 파싱\n",
    "args = parse_args_llama()\n",
    "\n",
    "# args 확인\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset, split and edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "def preprocess_function_gene_interaction(tokenizer, ignore_index=-100, max_length=32):\n",
    "    def preprocess_function(examples):\n",
    "        # Create prompts based on the gene interactions and label (-1 or 1)\n",
    "        prompts = [\n",
    "            f\"Does Gene {gene1} interact positively or negatively with Gene {gene2}?\"\n",
    "            for gene1, gene2 in zip(examples['Gene1'], examples['Gene2'])\n",
    "        ]\n",
    "        \n",
    "        # Convert label (-1 or 1) to completion strings (\"positive interaction\" or \"negative interaction\")\n",
    "        completion = [\n",
    "            f\"{'positive' if label == 1 else 'negative'} interaction.\" \n",
    "            for label in examples['label']\n",
    "        ]\n",
    "\n",
    "        # Encode instruction\n",
    "        instruction = f\"\\n\\n###\\n\\n\"\n",
    "        instruction = tokenizer.encode(instruction, add_special_tokens=False)\n",
    "\n",
    "        # Tokenize the prompts and completions\n",
    "        model_inputs = tokenizer(prompts, add_special_tokens=False)\n",
    "        labels = tokenizer(completion, add_special_tokens=False)\n",
    "\n",
    "        batch_size = len(examples['Gene1'])\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Add bos & eos token\n",
    "            sample_input_ids = [tokenizer.bos_token_id] + model_inputs[\"input_ids\"][i]\n",
    "            label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n",
    "\n",
    "            # Adjust length so that both input and label fit within max_length\n",
    "            p_max_length = max_length - len(label_input_ids) - len(instruction)\n",
    "            sample_input_ids = sample_input_ids[:p_max_length] + instruction\n",
    "\n",
    "            model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "            labels[\"input_ids\"][i] = [ignore_index] * len(sample_input_ids) + label_input_ids\n",
    "            model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "\n",
    "        # Padding\n",
    "        for i in range(batch_size):\n",
    "            sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "            label_input_ids = labels[\"input_ids\"][i]\n",
    "            \n",
    "            # Pad to max_length\n",
    "            model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (max_length - len(sample_input_ids)) + sample_input_ids\n",
    "            model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\"attention_mask\"][i]\n",
    "            labels[\"input_ids\"][i] = [ignore_index] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "            \n",
    "            # Convert to torch tensors\n",
    "            model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i])\n",
    "            model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i])\n",
    "            labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i])\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    return preprocess_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ad6137affe4738b3a075efcd2088d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming 'tokenizer' is already defined and 'gene_dataset' contains 'Gene1', 'Gene2', and 'label'\n",
    "gene_dataset = gene_dataset.map(\n",
    "    preprocess_function_gene_interaction(tokenizer=tokenizer, max_length=32),\n",
    "    batched=True,\n",
    "    batch_size=None,\n",
    "    remove_columns=[i for i in gene_dataset.column_names if i not in ['node_id']],\n",
    "    keep_in_memory=True,\n",
    "    writer_batch_size=10000,\n",
    "    num_proc=1,\n",
    ").with_format(\"torch\")\n",
    "\n",
    "# Now, `preprocessed_dataset` contains the tokenized inputs and labels ready for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['node_id', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 900\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Train, Validation, and Test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['node_id', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 540\n",
      "}) Dataset({\n",
      "    features: ['node_id', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 180\n",
      "}) Dataset({\n",
      "    features: ['node_id', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 180\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Split the dataset into 60% training, 20% validation, and 20% test\n",
    "train_test = gene_dataset.train_test_split(test_size=0.40, seed=42)\n",
    "val_test = train_test['test'].train_test_split(test_size=0.50, seed=42)\n",
    "\n",
    "# Create a DatasetDict to manage the splits\n",
    "split_datasets = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'val': val_test['train'],\n",
    "    'test': val_test['test']\n",
    "})\n",
    "\n",
    "# Convert to torch format\n",
    "split_datasets.set_format(\"torch\")\n",
    "\n",
    "# Now you have the train, validation, and test datasets\n",
    "train_dataset = split_datasets['train']\n",
    "val_dataset = split_datasets['val']\n",
    "test_dataset = split_datasets['test']\n",
    "\n",
    "# Print to check\n",
    "print(train_dataset, val_dataset, test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce the size of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['node_id', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 120\n",
      "}) Dataset({\n",
      "    features: ['node_id', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 40\n",
      "}) Dataset({\n",
      "    features: ['node_id', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 40\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Shuffle the datasets and select the required number of samples\n",
    "train_dataset = train_test['train'].shuffle(seed=42).select(range(120))\n",
    "val_dataset = val_test['train'].shuffle(seed=42).select(range(40))\n",
    "test_dataset = val_test['test'].shuffle(seed=42).select(range(40))\n",
    "\n",
    "# Convert to torch format\n",
    "train_dataset.set_format(\"torch\")\n",
    "val_dataset.set_format(\"torch\")\n",
    "test_dataset.set_format(\"torch\")\n",
    "\n",
    "# Print to check\n",
    "print(train_dataset, val_dataset, test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,\n",
       " tensor([812, 122,  31, 473, 522, 559, 690, 364, 481, 866, 836, 594, 526, 175,\n",
       "         254, 509, 292, 392, 362, 191,  93, 423, 456,  62, 732,  68, 539, 228,\n",
       "         642, 523,  41, 453, 563, 256,  72, 332, 333,  43, 284, 280, 730, 717,\n",
       "         404, 177,  75, 899, 373, 757, 465, 675,  99, 491, 606, 499, 841, 347,\n",
       "         262, 170, 414, 561, 466, 814, 187,  85, 827, 291, 645, 203, 403, 554,\n",
       "         747, 653, 234, 888, 480, 446, 489, 791, 512, 178, 849, 560, 113, 339,\n",
       "         134, 829,  19, 519, 867, 659, 524, 825, 189, 215, 468, 353, 873, 223,\n",
       "         438, 104, 736, 824, 282,  13, 430, 483,  32, 497,  63, 571, 616, 802,\n",
       "         195, 327, 821, 337, 343, 615,  95, 779]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset['node_id']), train_dataset['node_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,\n",
       " tensor([121,  44, 172, 676, 636, 845, 199, 813,  64, 565, 478, 729, 799, 236,\n",
       "         274, 880, 716, 592, 202, 107, 455, 881, 681, 159, 171, 248, 598, 626,\n",
       "         138, 434,  71, 803, 166, 643, 350, 147, 445, 860, 703, 241]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset['node_id']), test_dataset['node_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,\n",
       " tensor([[    0,     0,     0,  ..., 14881, 29889,     2],\n",
       "         [    0,     0,     0,  ..., 14881, 29889,     2],\n",
       "         [    0,     0,     0,  ..., 14881, 29889,     2],\n",
       "         ...,\n",
       "         [    0,     0,     0,  ..., 14881, 29889,     2],\n",
       "         [    0,     0,     0,  ..., 14881, 29889,     2],\n",
       "         [    0,     0,     0,  ..., 14881, 29889,     2]]),\n",
       " 32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset['input_ids']), train_dataset['input_ids'], len(train_dataset['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,\n",
       " tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1]]),\n",
       " 32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset['attention_mask']), train_dataset['attention_mask'], len(train_dataset['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,\n",
       " tensor([[ -100,  -100,  -100,  ..., 14881, 29889,     2],\n",
       "         [ -100,  -100,  -100,  ..., 14881, 29889,     2],\n",
       "         [ -100,  -100,  -100,  ..., 14881, 29889,     2],\n",
       "         ...,\n",
       "         [ -100,  -100,  -100,  ..., 14881, 29889,     2],\n",
       "         [ -100,  -100,  -100,  ..., 14881, 29889,     2],\n",
       "         [ -100,  -100,  -100,  ..., 14881, 29889,     2]]),\n",
       " 32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset['labels']), train_dataset['labels'], len(train_dataset['labels'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# # Load the Hugging Face LLaMA model (daryl149/llama-2-7b-chat-hf)\n",
    "# model_id = \"daryl149/llama-2-7b-chat-hf\"\n",
    "\n",
    "# # Initialize the model with FP16 precision and auto device map (for GPU if available)\n",
    "# if torch.cuda.is_available():\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "# else:\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Optional, Tuple, List, Union\n",
    "from dataclasses import dataclass, field\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch_geometric.utils\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from torch_geometric.nn.pool import global_max_pool, global_mean_pool\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_heads: int = 32\n",
    "    vocab_size: int = -1  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 4096\n",
    "\n",
    "    adapter_len: int = 0\n",
    "    adapter_layer: int = 0\n",
    "    adapter_dim: int = 512\n",
    "    adapter_n_heads: int = 4\n",
    "\n",
    "\n",
    "    num_hops: int = 2\n",
    "    w_adapter: bool = True\n",
    "    w_lora: bool = True\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 1\n",
    "    lora_dropout: float = 0.05\n",
    "    rrwp: int = 8\n",
    "\n",
    "\n",
    "    n_decoder_layers: int = 2\n",
    "    n_mp_layers: int = 2\n",
    "    n_encoder_layers: int = 2\n",
    "\n",
    "    # target_modules: Tuple[str] = ('q_proj', 'v_proj')     # Option\n",
    "    fans_out: Tuple[int] = (50, 50, 50)\n",
    "\n",
    "    # target_modules: Tuple[str] = ('q_proj', 'v_proj', 'k_proj')     # Option\n",
    "    # target_modules: Tuple[str] = ('o_proj')     # Option\n",
    "    target_modules: Tuple[str] = ('down_proj', 'up_proj', 'gate_proj')     # Option\n",
    "    task_level: str = 'node'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a44633574c49b094af53ae04e938c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaConfig\n",
    "import pert\n",
    "from peft import LoraConfig, get_peft_model\n",
    "# Load the LLaMA model and tokenizer\n",
    "model_id = \"daryl149/llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# ModelArgs 객체를 먼저 생성한 후, 속성을 수동으로 설정\n",
    "model_args = ModelArgs()\n",
    "\n",
    "# ModelArgs 속성 수동 설정 (Note: Do not change hidden_size or num_attention_heads)\n",
    "model_args.w_lora = False\n",
    "model_args.w_adapter = True\n",
    "model_args.adapter_layer = 8\n",
    "model_args.adapter_dim = 4096  # match the pre-trained model hidden size (4096 for LLaMA 7B)\n",
    "model_args.adapter_len = args.adapter_len\n",
    "model_args.lora_alpha = 16\n",
    "model_args.lora_r = 8\n",
    "model_args.num_hops = 3\n",
    "model_args.n_mp_layers = args.n_mp_layers\n",
    "model_args.rrwp = args.rrwp\n",
    "model_args.n_encoder_layers = args.n_encoder_layers\n",
    "model_args.n_decoder_layers = args.n_decoder_layers\n",
    "model_args.adapter_n_heads = 32  # Must match the pre-trained model attention heads (32 for LLaMA 7B)\n",
    "model_args.task_level = \"node_classification\"\n",
    "\n",
    "# Use the original configuration\n",
    "config = LlamaConfig.from_pretrained(model_id)\n",
    "\n",
    "# BFloat16 tensor\n",
    "torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)\n",
    "\n",
    "# Initialize the model with FP16 precision and auto device map (for GPU if available)\n",
    "if torch.cuda.is_available():\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, config=config)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, config=config)\n",
    "\n",
    "\n",
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=model_args.lora_r,  # Rank of the low-rank matrices\n",
    "    lora_alpha=model_args.lora_alpha,  # LoRA scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # LoRA 적용할 모듈들\n",
    "    lora_dropout=0.05,  # Dropout 적용\n",
    "    bias=\"none\"  # Bias 사용 여부 (none으로 설정)\n",
    ")\n",
    "\n",
    "# LoRA 적용\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Trainable 파라미터 확인 (LoRA 적용된 파라미터만 학습 가능하도록 설정)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# You can now proceed to add your adapter/LoRA layers on top of this model without changing its core architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"daryl149/llama-2-7b-chat-hf\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.43.4\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Create DateLoader for train, validation, evaluation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "# 인수 파싱\n",
    "args = parse_args_llama()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "generator = torch.Generator(device=device)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, drop_last=True,\n",
    "                                            shuffle=True, collate_fn=default_data_collator,\n",
    "                                           generator=generator)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, drop_last=False,\n",
    "                                          shuffle=False, collate_fn=default_data_collator,\n",
    "                                         generator=generator)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, drop_last=False,\n",
    "                                         shuffle=False, collate_fn=default_data_collator,\n",
    "                                          generator=generator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Freeze all layers except the adapter and LoRA layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Assuming adapter and LoRA layers are already added to the model, make those trainable\n",
    "def set_trainable_params_new(model):\n",
    "    param_adapter, param_lora = [], []\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'adapter' in name:  # Custom logic for adapter layers\n",
    "            param.requires_grad = True\n",
    "            param_adapter.append(param)\n",
    "        elif 'lora' in name:  # Custom logic for LoRA layers\n",
    "            param.requires_grad = True\n",
    "            param_lora.append(param)\n",
    "    return param_adapter, param_lora\n",
    "\n",
    "# Get the trainable parameters\n",
    "param_adapter, param_lora = set_trainable_params_new(model)\n",
    "\n",
    "# Step 4: Define optimizer for both adapter and LoRA layers\n",
    "lr_group = {\n",
    "    'adapter': 5e-5,  # Set your learning rate\n",
    "    'lora': 5e-5,     # Set your learning rate for LoRA layers\n",
    "}\n",
    "\n",
    "wd_group = {\n",
    "    'adapter': 0.01,  # Set weight decay\n",
    "    'lora': 0.01,     # Set weight decay for LoRA layers\n",
    "}\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {'params': param_adapter, 'lr': lr_group['adapter'], 'weight_decay': wd_group['adapter']},\n",
    "        {'params': param_lora, 'lr': lr_group['lora'], 'weight_decay': wd_group['lora']},\n",
    "    ],\n",
    "    betas=(0.9, 0.95)\n",
    ")\n",
    "\n",
    "# Training setup\n",
    "def adjust_learning_rate(param_group, LR, epoch, args):\n",
    "    # Custom function for learning rate adjustment\n",
    "    min_lr = 0.001 #5e-6\n",
    "    if epoch < args['warmup_epochs']:\n",
    "        lr = LR * epoch / args['warmup_epochs']\n",
    "    else:\n",
    "        lr = min_lr + (LR - min_lr) * 0.5 * (\n",
    "                1.0 + torch.cos(torch.tensor(epoch * 3.1415 / args['num_epochs'])))\n",
    "    param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2366a941d8ea4a30ad190d3189bc800c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7b502176c04dec9b60aa8b5cdc177b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished with average loss: 0.6278458298407107\n",
      "Validation loss for epoch 1: 0.18639653827995062\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db02e775fb7420594974016ea7528d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished with average loss: 0.2301026154619952\n",
      "Validation loss for epoch 2: 0.16073068361729384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9a6de0db4c4cf69c43271ad8501dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished with average loss: 0.14857714967026064\n",
      "Validation loss for epoch 3: 0.18001933582127094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916ba67f6e1946a2bfaadcb1da189167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished with average loss: 0.1400083626659883\n",
      "Validation loss for epoch 4: 0.0936630752752535\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2cc80652694479bed4faa41a62ced3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished with average loss: 0.1168101852100032\n",
      "Validation loss for epoch 5: 0.1415713634021813\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2e5524a0b04fc6b5b686214085da41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10 Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 finished with average loss: 0.09536855762187316\n",
      "Validation loss for epoch 6: 0.11126171628493467\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b5e0be95c64a39b7775f1c2df56a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10 Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 finished with average loss: 0.08546966973669139\n",
      "Validation loss for epoch 7: 0.028947628637979506\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3b8abdcb534542a53d1bb7b176b142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10 Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 finished with average loss: 0.06439952158614991\n",
      "Validation loss for epoch 8: 0.03667574356486512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3312fdf556bf425299441cec9bac51e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10 Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 finished with average loss: 0.04170702987379779\n",
      "Validation loss for epoch 9: 0.002915508874821171\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4d03fdd1344b4bba08b01d4e547c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10 Batches:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 finished with average loss: 0.029949112897619066\n",
      "Validation loss for epoch 10: 0.0022432866297094735\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Define arguments\n",
    "args = {'num_epochs': 10, 'warmup_epochs': 1, 'grad_steps': 2}\n",
    "lr_group = {'adapter': 1e-5, 'lora': 1e-5}  # Example learning rate settings\n",
    "\n",
    "# Training loop with tqdm for epoch and batch progress\n",
    "for epoch in tqdm(range(args['num_epochs']), desc=\"Epochs\"):\n",
    "    model.train()\n",
    "    epoch_loss, accum_loss = 0.0, 0.0\n",
    "\n",
    "    # Progress bar for each batch within an epoch\n",
    "    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch + 1}/{args['num_epochs']} Batches\") as batch_progress:\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Move batch to the device (ensure all tensors are on the same device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            if 'position_ids' in batch:\n",
    "                position_ids = batch['position_ids'].to(device)\n",
    "\n",
    "            # Forward pass and loss computation\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Adjust learning rates for adapter and LoRA layers\n",
    "            adjust_learning_rate(optimizer.param_groups[0], lr_group['adapter'], step + epoch, args)\n",
    "            adjust_learning_rate(optimizer.param_groups[1], lr_group['lora'], step + epoch, args)\n",
    "\n",
    "            # Update model parameters\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if (step + 1) % args['grad_steps'] == 0:\n",
    "                accum_loss = 0.0  # Reset accumulated loss after each gradient step\n",
    "\n",
    "            # Update the batch progress bar\n",
    "            batch_progress.set_postfix(Loss=loss.item())\n",
    "            batch_progress.update(1)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} finished with average loss: {epoch_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(val_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "        \n",
    "        print(f\"Validation loss for epoch {epoch + 1}: {val_loss / len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'new_trained_model/gene_trained_model_0.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4b577d850148f6b9556af884a2f618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.001986566302366555\n",
      "Test Accuracy: 85.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "samples_seen = 0\n",
    "eval_output = []\n",
    "eval_loss = 0.0\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "model.eval()\n",
    "\n",
    "# Progress bar for test loader\n",
    "progress_bar_test = tqdm(range(len(test_loader)), desc=\"Evaluating\")\n",
    "\n",
    "for step, batch in enumerate(test_loader):\n",
    "    with torch.no_grad():\n",
    "        # Move batch tensors to the device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Prepare model input\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        # The loss is already computed in the model outputs when you pass labels\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "        # Get predicted class (assuming the model outputs logits)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "        # Update accuracy\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "        # Gather predictions for later analysis\n",
    "        eval_output.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Update the progress bar for each batch\n",
    "    progress_bar_test.update(1)\n",
    "\n",
    "# Final evaluation metrics\n",
    "accuracy = correct_predictions / total_predictions\n",
    "average_loss = eval_loss / len(test_loader)\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Loss: {average_loss}\")\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Finish progress bar\n",
    "progress_bar_test.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
